video_dir: "to/your/Datasets/dev/" #upper leve directory where your datasets are stored
base_model_name: "LanguageBind/Video-LLaVA-7B-hf" 
adapter_name: mvrdock/Video-LLaVA-7B-natural-2000" #Null, mvrdock/Video-LLaVA-7B-natural
experiment_name: "O2" # corresponds to the folder name like O1,O2,O3 or gravity, etc
output_file_name: "base" 
dataset_type: "intphys" #intphys or inflevel
hf_home: "to/your/cache" # for caching
n_frames: 7 # How many frames you skipS
num_workers: 0
# for a full list of prompts, see prompts.yaml
prompt: "USER: <video>\n Do all objects in this video maintain their shape? Answer 'yes' or 'no' only. Assistant:"

# test type and cot only for inflevel
test_type: "binary" # COT, continuos or binary
cot_config:  #Only works if test type is "COT"
  LLM_model_name: "vicgalle/gpt2-open-instruct-v1"
  LLM_prompt: |
                ### Instruction:
                Answer with "yes" or "no" only. If the statement clearly indicates that **an object** skips or teleports, answer "no". If the statement suggests that **all objects** move without skipping or teleporting, answer "yes".
                Statment:
                {llava_output}
                ### Response:\n
experiment_config: #Only use for Inflevel, these are pretty much constant
  gravity:
    fields: ["camera_loc", "cover", "obj", "trial_type"]
  continuity:
    fields: ["camera_loc", "cover", "obj", "trial_type", "dir"]
  solidity:
    fields: ["camera_loc", "cover", "obj", "trial_type"]
  custom:
    fields: ["tbd", "tbd2"]